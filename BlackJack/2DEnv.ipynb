{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLACKJACK GAME USING Q LEARNING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all know about the game Black Jack game also known to be 21 a card game where players try to get closer to 21 than the dealer and \n",
    "\n",
    "the main goal is \n",
    "Players try to have a higher hand value than the dealer without going over 21.\n",
    "Cards\n",
    "\n",
    "Face cards (Jack, Queen, King) are worth 10, Aces are worth 1 or 11, and other cards are worth their face value.\n",
    "\n",
    "Dealing\n",
    "Players receive two cards face up, and the dealer receives two cards, one face up and one face down. \n",
    "\n",
    "Playing\n",
    "Players can choose to \"hit\" and receive more cards or \"stand\" and keep their current hand. \n",
    "\n",
    "Winning\n",
    "If a player's hand is closer to 21 than the dealer's, they win. If the dealer's hand goes over 21, all players win. If neither the player nor the dealer goes over 21, the player with the higher hand wins.\n",
    "\n",
    "in case there is a tie  \n",
    "Ties\n",
    "If neither the player nor the dealer goes over 21, it's a tie, or \"push\", and the bet remains on the table. \n",
    "\n",
    "Blackjack\n",
    "If a player's first two cards total 21, it's called a \"blackjack\" or \"natural\" and is the strongest hand. If a player has a blackjack, their bet is multiplied by 3.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Q Learning works in Black Jack\n",
    "As, Q learning is one of the reinforcement learning algorithm where no model is required where it finds the best optimal action selection policy for a finite MDP where it helps an agent to maximize the reward over time through repeated interaction with the environment even though the model is not known \n",
    "\n",
    "therefore it is called to be as \n",
    "Model-Free Approach where \n",
    "No Model Required: Q-learning is a model-free algorithm, which means it does not require a model of the environment (i.e., it does not need to know the transition probabilities and reward functions). This makes it particularly useful in environments where the dynamics are unknown or difficult to model.\n",
    "\n",
    "\n",
    "Direct Learning from Experience: The agent learns optimal policies directly from interactions with the environment through trial and error without needing to construct or infer a model.\n",
    "\n",
    "\n",
    "\n",
    "## Working :\n",
    "In this algorithm it maintains Q values for each state -action pair ,representing the expected utility of taking an given action in a given state following the optimal policy The Q-values are initialized arbitrarily and are updated iteratively using the experiences gathered by the agent.\n",
    "\n",
    "2. Q-value Update Rule: The Q-values are updated using the formula:\n",
    "\n",
    "𝑄(𝑠,𝑎)←𝑄(𝑠,𝑎)+𝛼[𝑟+𝛾max𝑎′𝑄(𝑠′,𝑎′)−𝑄(𝑠,𝑎)]\n",
    "\n",
    "The Q-values are initialized arbitrarily and are updated iteratively using the experiences gathered by the agent.\n",
    "\n",
    "where :\n",
    "\n",
    "𝑠 is the current state.\n",
    "\n",
    "𝑎 is the action taken.\n",
    "\n",
    "r is the reward received after taking \n",
    "action 𝑎 in state 𝑠.\n",
    "\n",
    "𝑠′ is the new state after action.\n",
    "\n",
    "𝑎′ is any possible action from the new state 𝑠′.\n",
    "\n",
    "𝛼 is the learning rate (0 < α ≤ 1).\n",
    "\n",
    "𝛾 is the discount factor (0 ≤ γ < 1).\n",
    "\n",
    "\n",
    "defining the policy :\n",
    "policy determines what action to be taken in each state and that can be derived from the Q values also known to be as Action value of each state ,the policy chooses the action with the highest Q-value in each state (exploitation), though sometimes a less optimal action is chosen for exploration purposes.\n",
    "\n",
    "\n",
    "4.Exploration vs Exploitation :  Q-learning manages the trade-off between exploration (choosing random actions to discover new strategies) and exploitation (choosing actions based on accumulated knowledge). Techniques like the epsilon-greedy strategy, where the agent mostly takes the best-known action but occasionally tries a random action, often manage the balance between these.\n",
    "\n",
    "\n",
    "Convergence: Under certain conditions, such as ensuring all state-action pairs are visited an infinite number of times, Q-learning converges to the optimal policy and Q-values that give the maximum expected reward for any state under any conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player busts!\n",
      "Player busts!\n",
      "Dealer wins!\n",
      "Dealer wins!\n",
      "Dealer wins!\n",
      "Dealer hits.\n",
      "Player wins!\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import os\n",
    "from Blackjack1 import BlackjackEnv, Card\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Screen dimensions\n",
    "WIDTH, HEIGHT = 800, 600\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"2D Blackjack Game\")\n",
    "\n",
    "# Colors\n",
    "GREEN = (34, 139, 34)\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "BUTTON_COLOR = (0, 128, 0)\n",
    "BUTTON_TEXT_COLOR = (255, 255, 255)\n",
    "\n",
    "# Load card images\n",
    "CARD_WIDTH, CARD_HEIGHT = 100, 150\n",
    "card_images = {}\n",
    "card_dir = \"D:/GitHub/reinforcement_learning_project/cards\"  # Update with the correct path to your card images\n",
    "\n",
    "for suit in [\"clubs\", \"diamonds\", \"hearts\", \"spades\"]:\n",
    "    for rank in [\"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"jack\", \"queen\", \"king\", \"ace\"]:\n",
    "        card_name = f\"{rank} of {suit}.jpg\"\n",
    "        card_path = os.path.join(card_dir, card_name)\n",
    "        card_images[f\"{rank} of {suit}\"] = pygame.transform.scale(pygame.image.load(card_path), (CARD_WIDTH, CARD_HEIGHT))\n",
    "\n",
    "# Initialize Blackjack environment\n",
    "env = BlackjackEnv()\n",
    "\n",
    "def draw_card(card, x, y):\n",
    "    \"\"\"Draw a card on the screen.\"\"\"\n",
    "    card_key = f\"{card.rank} of {card.suit}\"\n",
    "    screen.blit(card_images[card_key], (x, y))\n",
    "\n",
    "def draw_hand(hand, x, y):\n",
    "    \"\"\"Draw a player's hand.\"\"\"\n",
    "    for i, card in enumerate(hand.cards):\n",
    "        draw_card(card, x + i * 30, y)\n",
    "\n",
    "def draw_table(player_hand, dealer_hand, dealer_hide_second=False, message=None):\n",
    "    \"\"\"Render the table with player and dealer hands and UI elements.\"\"\"\n",
    "    screen.fill(GREEN)\n",
    "    \n",
    "    # Draw dealer's hand\n",
    "    if dealer_hide_second:\n",
    "        draw_card(dealer_hand.cards[0], 300, 50)\n",
    "        pygame.draw.rect(screen, BLACK, pygame.Rect(330, 50, CARD_WIDTH, CARD_HEIGHT))\n",
    "    else:\n",
    "        draw_hand(dealer_hand, 300, 50)\n",
    "\n",
    "    # Draw player's hand\n",
    "    draw_hand(player_hand, 300, 400)\n",
    "    \n",
    "    # Display player value\n",
    "    font = pygame.font.Font(None, 36)\n",
    "    player_text = font.render(f\"Player Value: {player_hand.value()}\", True, WHITE)\n",
    "    screen.blit(player_text, (50, 500))\n",
    "\n",
    "    # Display message if any\n",
    "    if message:\n",
    "        message_text = font.render(message, True, WHITE)\n",
    "        screen.blit(message_text, (WIDTH // 2 - message_text.get_width() // 2, HEIGHT // 2))\n",
    "\n",
    "    # Draw Hit and Stick buttons\n",
    "    hit_button = pygame.Rect(100, 520, 100, 40)\n",
    "    stick_button = pygame.Rect(600, 520, 100, 40)\n",
    "    pygame.draw.rect(screen, BUTTON_COLOR, hit_button)\n",
    "    pygame.draw.rect(screen, BUTTON_COLOR, stick_button)\n",
    "    hit_text = font.render(\"Hit\", True, BUTTON_TEXT_COLOR)\n",
    "    stick_text = font.render(\"Stick\", True, BUTTON_TEXT_COLOR)\n",
    "    screen.blit(hit_text, (hit_button.centerx - hit_text.get_width() // 2, hit_button.centery - hit_text.get_height() // 2))\n",
    "    screen.blit(stick_text, (stick_button.centerx - stick_text.get_width() // 2, stick_button.centery - stick_text.get_height() // 2))\n",
    "\n",
    "    # Draw Reset button\n",
    "    reset_button = pygame.Rect(WIDTH - 110, 10, 100, 40)\n",
    "    pygame.draw.rect(screen, BUTTON_COLOR, reset_button)\n",
    "    reset_text = font.render(\"Reset\", True, BUTTON_TEXT_COLOR)\n",
    "    screen.blit(reset_text, (reset_button.centerx - reset_text.get_width() // 2, reset_button.centery - reset_text.get_height() // 2))\n",
    "\n",
    "    return hit_button, stick_button, reset_button\n",
    "\n",
    "def check_for_result(player_hand, dealer_hand):\n",
    "    \"\"\"Check the result of the game (win/loss/bust).\"\"\"\n",
    "    player_value = player_hand.value()\n",
    "    dealer_value = dealer_hand.value()\n",
    "\n",
    "    if player_value > 21:\n",
    "        return \"Player Busted!\"\n",
    "    elif dealer_value > 21:\n",
    "        return \"Dealer Busted!\"\n",
    "    elif player_value > dealer_value:\n",
    "        return \"You Win!\"\n",
    "    elif player_value < dealer_value:\n",
    "        return \"Dealer Wins!\"\n",
    "    else:\n",
    "        return \"It's a Tie!\"\n",
    "\n",
    "def main():\n",
    "    clock = pygame.time.Clock()\n",
    "    state = env.reset()\n",
    "    running = True\n",
    "    done = False\n",
    "    message = None\n",
    "\n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                mouse_x, mouse_y = event.pos\n",
    "                hit_button, stick_button, reset_button = draw_table(env.player_hand, env.dealer_hand, dealer_hide_second=not done, message=message)\n",
    "\n",
    "                if not done:\n",
    "                    if hit_button.collidepoint(mouse_x, mouse_y):\n",
    "                        state, _, done = env.step(1)  # Hit\n",
    "                    elif stick_button.collidepoint(mouse_x, mouse_y):\n",
    "                        state, _, done = env.step(0)  # Stick\n",
    "                # Reset button behavior\n",
    "                if reset_button.collidepoint(mouse_x, mouse_y):\n",
    "                    state = env.reset()\n",
    "                    done = False\n",
    "                    message = None\n",
    "\n",
    "        if done:\n",
    "            # Check for result after the game ends\n",
    "            message = check_for_result(env.player_hand, env.dealer_hand)\n",
    "\n",
    "        draw_table(env.player_hand, env.dealer_hand, dealer_hide_second=not done, message=message)\n",
    "        pygame.display.flip()\n",
    "        clock.tick(30)\n",
    "\n",
    "    pygame.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
